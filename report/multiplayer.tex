Before starting to implement our multiplayer AI we need to justify it's creation. We therefore need to answer why not just calculate the optimal strategy for two player \emph{yahtzee} by analysing the whole state space, since \emph{yahtzee} is borderline calculable by modern computers. We can analyse the state space for the optimal two player strategy the same way we did with single player \emph{yahtzee}. First we have 13 bit for each player to keep track of their scoreboards, 6 bit per player for keeping track of their upper section score, 10 bits for keeping track of the difference between the two players score and 1 bit for keeping track of what players turn it is. Therefore the amount of different states is at most $26+12+10+1 = 49\mbox{bits}$ if we don't consider the existence of unreachable states. This would therefore take naively $2^{49-19} = 2^{30}$ times longer to calculate than the single player AI. Also even in the case of the states only requiring 1 byte of memory the whole cache would still fill $562 \mbox{TB}$ which have to be within memory when running the AI\st{which have to be within memory when running the AI.}. 

In conclusion even an optimized algorithm would require an enormous setup for calculating the optimal two player strategy. We strive to make an AI which can be run and computed on most modern computers and perform better than single player AI by considering information about it's opponent. Our implementation uses the same dynamic programming as single player AI, that is it calculates a cache containing the values of the different states. The difference is that we calculate many caches for different degrees of aggression levels, referred to as strategies. The strategy is then changed depending on the difference between the AI and the opponent expected score\footnote{Expected score is found by running single player AI for the opponents current state}. An aggression level can be either negative or positive depending on whether the AI should play aggressively because it is far behind or passively because it is winning. The strategy is chosen by finding the probability of beating the other players expected score. 

This probability can be found using the \emph{cumulative normal distribution} which describes the chance of beating score $a$ given score $b$ and a standard deviation in  $\sigma$. %TODO write proper formula for cummulative normal distribution%

$\newline\mbox{Cummulative normal distribution:} \newline y = 1  /(\sigma * \sqrt{2 * \pi} * e^{-(x - \upsilon)^2 / 2 \sigma^2}$)\newline

Here $a$ is the expected score of the opponent and $b$ are the expected score in a given state for each of our strategies. The \emph{standard deviation} $\sigma$ expresses the variation from the average value of a random variable. It can be computed from knowing the combined \emph{variance} in scores for each state. 

$\newline\mbox{Standard deviation:} \newline \sigma = \sqrt{\mbox{var}} $
$\newline \mbox{Variance:} \newline var = p_i * \sum_{i=1}^{k} \newline\left[s_i^2 + (m_i^2 * (1 - p_i)) - \sum_{j=i}^{k} \left[p_i * p_j * m_i * m_j\right]\right]$\newline

%TODO Explain formula for standard deviation.%
The standard deviation should be saved for each state in the big dynamic search alongside the expected score in each cache/strategy. We can now use the cumulative normal distribution to calculate for each strategy the probability of winning the game for our current state. The one with the highest chance is chosen as the cache to be used for the current turn.

What we achieve is a AI that instead of maximizing it's own score focuses on increasing the probability of winning the game, which is the optimal strategy. The tradeoff is that our strategy is based on a alot of approximate information and therefore only provides an approximation of what the actual optimal strategy is. The standard deviation for a given state is only an approximation of what the full normal distribution for that state is\footnote{Calculating the normal distribution for every possible score for each state takes more time to calculate than what is feasible.}, meaning our algorithm in nature relies on an approximation of the probability of winning the game. Another approximate is that the AI performs better depending on the number of strategies/caches there are calculated. Say as an example that the optimal aggression is $0,53$ but we only have the caches calculated for $0,6$ and $0,3$. We then choose the cache with aggression value closest to $0,53$, that is $0,6$. Last we use the solitaire AI as an estimate of the final expected score, which is correct when playing against the solitaire AI but incorrect in any other instance.

We wish to show through testing that even if we use this approximated data we still have an AI which is capable of significantly beat the solitaire AI.

Multi player content
* Start med kort motivation der beskriver optimal (at det tager for laaang tid)
* Beskriv near optimal
* Husk at det er 2 player (evt. beskriv kort hvorfor ikke 3)
