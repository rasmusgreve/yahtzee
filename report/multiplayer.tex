Before starting to implement our multiplayer AI we need to justify it's creation. We therefore need to answer why not instead calculate the optimal strategy for two player \emph{Yahtzee} by analysing the whole state space. the solution to optimal two player \emph{Yahtzee} can be found through the same tree search we performed when calculating solitaire \emph{yahtzee} but adapted to the two player case. First we have 13 bit for each player to keep track of their scoreboards, 6 bit per player for keeping track of their upper section score, 10 bits for keeping track of the difference between the two players score and 1 bit for keeping track of what players turn it is. Therefore the amount of different states is at most $26+12+10+1 = 49\mbox{bits}$ if we don't consider the existence of unreachable states. This would naively take $2^{49-19} = 2^{30}$ times longer to calculate than the single player AI. Also even in the case of the states only requiring 1 byte of memory the whole cache would still fill $562 \mbox{TB}$\st{which have to be within memory when running the AI.}. In conclusion even an optimized algorithm would require an enormous setup for calculating the optimal two player strategy which conflicts with our goal to make an AI which can be run and computed on most modern computers. 

Our implementation uses the same dynamic programming as single player AI, that is it calculates a cache containing the values of the different states. The difference is that we calculate many caches for different degrees of aggression levels, referred to as strategies. The strategy is then changed depending on the difference between the AI and the opponent expected score\footnote{Expected score is found by running single player AI for the opponents current state}. An aggression level can be either negative or positive depending on whether the AI should play aggressively because it is far behind or passively because it is winning. The strategy is chosen by finding the probability of beating the other players expected score. 

This probability can be found using the \emph{cumulative normal distribution} which describes the chance of beating score $a$ given score $b$ and a standard deviation $\sigma$. %TODO write proper formula for cummulative normal distribution%

$\newline\mbox{Cummulative normal distribution:} \newline y = 1  /(\sigma * \sqrt{2 * \pi} * e^{-(x - \upsilon)^2 / 2 \sigma^2}$)\newline

Here $a$ is the expected score of the opponent and $b$ are the expected score in a given state for each of our strategies. The \emph{standard deviation} $\sigma$ expresses the variation from the average value of a random variable. In our implementation this random variable is our expected score. $\sigma$ can be computed from knowing the \emph{variance}($\mbox{var}$) in score. Variance is the measurement of the spread in value range between $k$ given values is.

%TODO write proper formula for standard deviation%
$\newline\mbox{Standard deviation:} \newline \sigma = \sqrt{\mbox{var}} $
$\newline \mbox{Variance:} \newline var = p_i * \sum_{i=1}^{k} \newline\left[s_i^2 + (m_i^2 * (1 - p_i)) - \sum_{j=i}^{k} \left[p_i * p_j * m_i * m_j\right]\right]$\newline
%TODO Explain formula for standard deviation.%

The standard deviation should be saved for each state in the \emph{big dynamic search} alongside the expected score in each cache/strategy. We can now use the cumulative normal distribution to calculate for each strategy the probability of winning the game for our current state. The strategy with the highest chance is chosen as the cache to be used for the current turn.

What we achieve is a AI that instead of maximizing it's own score focuses on increasing the probability of winning the game. The tradeoff is that our strategy is based on a lot of approximate information and therefore only provides an approximation of what the actual optimal strategy is. The standard deviation for a state is only an approximation of what the full normal distribution is for that state \footnote{Calculating the normal distribution for every possible score for each state takes more time to calculate than what is feasible.}, meaning our algorithm in nature relies on an approximation of the probability of winning the game. Another approximate is that the AI performs better depending on the number of strategies/caches there are calculated. Say as an example that the optimal aggression is $0,53$ but we only have the caches calculated for $0,6$ and $0,3$. We then choose the cache with the aggression value closest to $0,53$, that is $0,6$. Last we use the solitaire AI as an estimate of the final expected score, which is correct when playing against the solitaire AI but incorrect in any other instance.

We wish to show through testing that even if we use this approximated data we still have an AI which is capable of beating the solitaire AI significantly.

Multi player content
* Start med kort motivation der beskriver optimal (at det tager for laaang tid)
* Beskriv near optimal
* Husk at det er 2 player (evt. beskriv kort hvorfor ikke 3)
